{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本语法\n",
    "\n",
    "最经典的： 提示词模版 | 大模型 | 输出\n",
    "\n",
    "chain.invoke({\"key\", \"value\"}) 去触发， 并可以填充模版中的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"我 爱你 {name}\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser            # 加了数据解析器， 是AIMessage对象： <class 'langchain_core.messages.ai.AIMessage'>\n",
    "chain2 = prompt | model                           # 没有加数据解析器， 是字符串： <class 'str'>\n",
    "\n",
    "print(type(chain2.invoke({\"name\": \"小明\"})))\n",
    "print(type(chain.invoke({\"name\": \"小明\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要组件\n",
    "\n",
    "### Prompt Templates\n",
    "用于构造提示词模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='你好，世界！' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 35, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Baar6AGIR0BAL9gP9bORGgWJ2e64d', 'finish_reason': 'stop', 'logprobs': None} id='run-a88d7660-339f-4542-b394-40f99d19820e-0' usage_metadata={'input_tokens': 35, 'output_tokens': 7, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 简单模板\n",
    "prompt = ChatPromptTemplate.from_template(\"翻译以下文本：{text}\")\n",
    "\n",
    "# 多消息模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个专业的翻译助手\"),\n",
    "    (\"user\", \"翻译：{text}\")\n",
    "])\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = prompt | model\n",
    "response = chain.invoke({\"text\": \"Hello, world!\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "支持各种语言模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Output Parsers\n",
    "\n",
    "解析模型输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# 字符串解析器\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "# JSON解析器\n",
    "json_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高级特性\n",
    "### 1. 并行执行\n",
    "使用 RunnableParallel 实现并行处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableParallel\n\u001b[32m      3\u001b[39m parallel_chain = RunnableParallel({\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mprompt1\u001b[49m | model | StrOutputParser(),\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33manalysis\u001b[39m\u001b[33m\"\u001b[39m: prompt2 | model | StrOutputParser()\n\u001b[32m      6\u001b[39m })\n",
      "\u001b[31mNameError\u001b[39m: name 'prompt1' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": prompt1 | model | StrOutputParser(),\n",
    "    \"analysis\": prompt2 | model | StrOutputParser()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 条件分支\n",
    "使用 RunnableBranch 实现条件逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"question\" in x, question_chain),\n",
    "    (lambda x: \"summary\" in x, summary_chain),\n",
    "    default_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lambda函数\n",
    "在链条中插入自定义逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def custom_processor(input_dict):\n",
    "    # 自定义处理逻辑\n",
    "    return processed_data\n",
    "\n",
    "chain = (\n",
    "    prompt \n",
    "    | model \n",
    "    | RunnableLambda(custom_processor)\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实际应用示例\n",
    "#### 1. 简单翻译链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_chain = (\n",
    "    ChatPromptTemplate.from_template(\"将以下中文翻译成英文：{chinese_text}\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = translation_chain.invoke({\"chinese_text\": \"你好世界\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG（检索增强生成）链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"基于以下上下文回答问题：\\n\\n{context}\\n\\n问题：{question}\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 多步骤分析链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_chain = (\n",
    "    # 首先保存原始输入\n",
    "    RunnableParallel(\n",
    "        sentiment_analysis=ChatPromptTemplate.from_template(\"分析以下文本的情感：{text}\") | model | StrOutputParser(),\n",
    "        original_text=RunnablePassthrough()\n",
    "    )\n",
    "    # 现在有了 {\"sentiment_analysis\": \"积极\", \"original_text\": {\"text\": \"我很开心\"}}\n",
    "    \n",
    "    | RunnableLambda(lambda x: {\n",
    "        \"sentiment\": x[\"sentiment_analysis\"], \n",
    "        \"original_text\": x[\"original_text\"][\"text\"]\n",
    "    })\n",
    "    \n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"基于情感分析结果：{sentiment}，为原文本提供改进建议：{original_text}\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异步支持\n",
    "LCEL 支持异步操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_async_chain():\n",
    "    result = await chain.ainvoke({\"topic\": \"programming\"})\n",
    "    return result\n",
    "\n",
    "# 批处理\n",
    "async def batch_process():\n",
    "    inputs = [{\"topic\": \"AI\"}, {\"topic\": \"ML\"}, {\"topic\": \"DL\"}]\n",
    "    results = await chain.abatch(inputs)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流式处理\n",
    "支持流式输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同步流\n",
    "for chunk in chain.stream({\"topic\": \"technology\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# 异步流\n",
    "async for chunk in chain.astream({\"topic\": \"technology\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boss_find_job",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
